---
author: Michael DeCrescenzo
categories: [code, r]
title: "`currr`: a functional grammar for data frame operations."
subtitle: Functional, composable, and deferred evaluation.
# subtitle: Lazyverse
# excerpt
date: "2023-10-03"
knitr:
    opts_chunk:
        eval: true
        include: true
        collapse: true
draft: false
---

```{r, delay}
#| include: false
#| eval: false
# curry a verb
# returns a function (...) -> (data -> data) which curries the provided verb.
# the innermost function assumes the data provided 
#    is the first arg to the fully specified verb call
currify_verb <- function(verb) {
    function(...) {
        intention <- function(.data) {
            return(purrr::partial(verb)(.data, ...))
        }
        memoise::memoise(intention)
    }
}
```


```{r, composition}
#| include: false
#| eval: false
`%.%` = function(g, f) function(...) g(f(...))
`%;%` = function(f, g) function(...) g(f(...))
```

```{r, interfaces}
#| include: false
#| eval: false
filtering = currify_verb(filter)
mutating = currify_verb(mutating)
summarizing = currify_verb(summarize)
grouping = currify_verb(group_by)
ungrouping = currify_verb(ungroup)
```

This post proposes a dialect for data frame operations that I call `currr`.
(I am not married to the name.)
You can find and install a (very unfinished) package on [Github](https://github.com/mikedecr/currr).

`currr` is built on `dplyr`, but it uses a distinct grammar.

A typical `dplyr` workflow pipes a data frame through a series of functions
(in this case `filter`, `group_by`, and `summarize`).

```{r}
#| message: false
library("palmerpenguins")
library("dplyr")

penguins |>
    filter(sex %in% c("male", "female")) |>
    group_by(sex, island) |>
    summarize(
        mass_med = median(body_mass_g, na.rm = TRUE),
        mass_mean = mean(body_mass_g, na.rm = TRUE),
        mass_std = sd(body_mass_g, na.rm = TRUE),
        .groups = "drop"
    )
```

The purpose of `currr` is to provide a convenient way to _pre-define_ functions that perform these data manipulation steps, without referring to a data frame.
Instead of `filter`, `group_by`, and `summarize`, here we use `filtering`, `grouping`, and `summarizing` to create _new functions_.


```{r, currr-functions-example}
library(currr)

# each of these is a function of a data frame
flt_sex_FM = filtering(sex %in% c("male", "female"))
by_sex_isl = grouping(sex, island)
smz_mass = summarizing(
    mass_med = median(body_mass_g, na.rm = TRUE),
    mass_mean = mean(body_mass_g, na.rm = TRUE),
    mass_std = sd(body_mass_g, na.rm = TRUE),
    .groups = "drop"
)
```

These functions can be rearranged, composed, and called later.

```{r, call}
(smz_mass %.% by_sex_isl)(penguins)

(smz_mass %.% by_sex_isl %.% flt_sex_FM)(penguins)
```

At this point you may be wondering, 

> If `dplyr` is already so good, why do I need this confusing, tedious style?

and I intend to give you a thorough answer as the post proceeds.

This post proceeds in three sections:

1. **The tl;dr**: how to use and understand `currr`
2. **For the skeptics**: why you would want to use `currr`.
3. **For the dorks**: how `currr` really works.


# The basics of `currr`

If you know `dplyr`, you know almost everything you need to know about `currr`.
There are only two things going on here.

1. `currr` code is just curried `dplyr` code.
1. `currr` functions are meant to be composed without evaluating them immediately.

### `currr` code is just curried `dplyr` code.

"Currying" a function means turning a function of many arguments into a function of fewer arguments.
I have [written about this before](https://mikedecr.netlify.app/blog/partial_fns_ggplot/), but we will explain it plenty here too.^[
    If `add(1, 2)` returns `3`, then `curry(add, 1)` creates a new function that adds an argument to `1`.
]

`dplyr` functions like `filter` and `select` have analogous `currr` functions like `filtering` and `selecting`.
The `currr` functions create _curried_ versions of the `dplyr` functions.
Let me give you an example.
Here is how we can use `dplyr::filter` to keep only known male and female penguins in the `palmerpenguins` dataset.

```{r, dplyr-filter}
filter(penguins, sex %in% c("male", "female"))
```

Notice that `filter` has several arguments: the data frame itself, and as many boolean expressions as you want to filter with.
And it returns a new data frame.

Here is how to accomplish a similar functionality with `currr::filtering`.
`filtering` is just like `filter`, but I don't pass the data frame.

```{r, currr-filtering-no-data}
filtering(sex %in% c("male", "female"))
```

And instead of getting a data frame back, I get a _new function_.
That resulting function would take me from data frame to data frame, but I can pass that data frame some time later.
I have _curried_ the `filter` function with the argument `sex %in% c("male", "female")`.
In other words, I created a version of the `filter` function with that boolean condition pre-specified.
When I call that function later, it will invoke the boolean expression without me specifying it later.

To assure you how it works, I will build the same function _and then_ pass the `penguins` data to the curried function.
I will get the same result as the original, fully-specified `filter` call.

```{r, currr-filtering-with-data}
flt_sex = filtering(sex %in% c("male", "female"))
flt_sex(penguins)
```

You typically don't want to pass the data immediately after creating the function.
The purpose of `currr` is to create the curried function and evaluate it on data later on.
Why delay the evaluation of data?
Because by _separating the function from the data_, it is easier to compose and recycle functionality and multiple datasets.
More explanation in the "Why" section below.


### `currr` functions can be composed without evaluating on data.

Most R uders hear "function composition" and think of the pipe operator `|>`.
The pipe operator turns a nested function call like `h(g(f(x)))` into something like `x |> f() |> g() |> h()`.
That is convenient, but its behavior is too _eager_ for our needs.
We want to separate the `x` from the `f`, `g`, and `h`.

I could get around this limitation by writing a new function that combines `f`, `g`, and `h` and then passing `x` to that function like this...

```{r}
#| eval: false
fgh = function(...) f(...) |> g() |> h()

x |> fgh()
```

...but it is a bit clunky
Mathemematical notation for function composition, meanwhile, is simple: $fgh = h \cdot g \cdot f$.^[
    You can read more about function composition in [this other post](https://mikedecr.netlify.app/blog/composition/) as well.
]

`currr` provides operators to two function composition operators to make this easy.
First, the `%.%` operator for classical mathematical composition: `(g %.% f)(x)` is like `g(f(x))`.
Second, a postfix-style `%;%` composition operator that reads more "pipe-like": `(f %;% g)(x)` is like `x |> f() |> g()`.^[
    There is also `purrr::compose`, which takes a vector of functions and an optional `.dir` argument to switch the direction of composition.
    Take your pick.
]

These operators let us take little functions and combine them without evaluating on data.
Take the example at the top of the post. These are functions.

```{r, currr-functions-example}
#| eval: false
```

I compose them into a "chain" of operations (really, a new function) without evaluating them.

```{r, currr-functions-comp}
mass_by_sex_isl = (flt_sex_FM %;% by_sex_isl %;% smz_mass)
```

And only when I need the results, I can pass the data.

```{r, currr-functions-eval}
mass_by_sex_isl(penguins)
```

# Why use the `currr` style?

Now that you know how to use `currr`, you may want to know why you would bother.

First, **pipe chains are easy to write but hard to re-use**.
Let's look at an example in `dplyr` world.
I have the `penguins` and I want to summarize the body mass column.

```{r}
penguins |>
    summarize(
        mass_med = median(body_mass_g, na.rm = TRUE),
        mass_mean = mean(body_mass_g, na.rm = TRUE),
        mass_std = sd(body_mass_g, na.rm = TRUE),
        .groups = "drop"
    )
```

Okay, I also want to compute the same stats, grouped by `sex`.

```{r}
penguins |>
    group_by(sex, island) |>
    summarize(
        mass_med = median(body_mass_g, na.rm = TRUE),
        mass_mean = mean(body_mass_g, na.rm = TRUE),
        mass_std = sd(body_mass_g, na.rm = TRUE),
        .groups = "drop"
    )
```

Notice I had to write the `summarize` step all over again.
And if I also wanted it grouped by sex _and_ island...

```{r}
penguins |>
    group_by(sex, island) |>
    summarize(
        mass_med = median(body_mass_g, na.rm = TRUE),
        mass_mean = mean(body_mass_g, na.rm = TRUE),
        mass_std = sd(body_mass_g, na.rm = TRUE),
        .groups = "drop"
    )
```

Now I have three instances in my code where I need to write the same `summarize` code, because I wanted to see it three different ways.
And if I want to change that code (say, add a mean abs. deviation statistic), I have to change it in multiple places.
This is a good reason to use a function!
`currr` gives us a way write that function conveniently and composably.
We write the `summarize` step one time and re-use whenever we want later.

```{r}
smz_mass = summarizing(
    mass_med = median(body_mass_g, na.rm = TRUE),
    mass_mean = mean(body_mass_g, na.rm = TRUE),
    mass_std = sd(body_mass_g, na.rm = TRUE),
    .groups = "drop"
)

smz_mass(penguins)

penguins |> group_by(sex) |> smz_mass()

penguins |> group_by(sex, island) |> smz_mass()
```

This is the benefit we get by _separating the data from the functionality_.

- When functions are pre-defined, we can pass whatever data whenever we want.
- It is easier to abstract over the data because we wrote modular functions instead of a bundled pipeline.
- We pay a little up-front cost to store the `dplyr` verb as a function, but we amortize the cost of repeated invocations of the same functionality.
  Lots of little functions may look silly in isolation, but they play important roles when composed with other functions.
- We greatly reduce the cost of changing key functionality; change the function definition in one place, and inherit the change everywhere the function is called.


# How `currr` works

`currr` works by creating curryable `dplyr` verbs.
For example:

```{r, interfaces}
#| include: true
#| eval: false
```

The implementation of `currify_verb` is, in turn...

```{r, delay}
#| include: true
#| eval: false
```

The outermost function `currify_verb` takes a `verb` and returns a new function of args `...`.
The args are then used to return _another function_ of `.data`, enclosing both the verb and the args in the environment of the innermost function.
I call the innermost function an `intention`, because it declares an intent to evaluate a verb without actually evaluating it (yet).

Another notable detail is that we return not the `intention` object but a memoized `intention` using the [memoise](https://memoise.r-lib.org/) package.
_Memoization_ associates a function value with the arguments that created that value, and caches the result.
This way, if a function is ever called again with the same arguments as before, we return the cached value associated with those arguments instead of recomputing the value anew.

Memoization is commonly used in functional programming because we want to express values as the result of functions.
If data have to pass through intermediate states, we want that state to be handled inside of functions instead of sitting in the open in main environment of the program.
Contrast this to `dplyr`, where we would avoid recomputation by storing a copy of our data in some intermediate state that we can use as a midway point to branch out from.
Memoization lets us get all the efficiency of intermediate data without polluting our program environment with stateful data that we don't care about.
Instead, we are only responsible for defining and composing the functionality that we want to invoke to transform our raw data.
Now that's functional programming!

The only other thing to note is how we implement function composition:

```{r, composition}
#| include: true
#| eval: false
```

In R, we can define our own binary operations as functions of two arguments.
Function composition is associative, so in the case of these binary operators, it is sufficient to define composition on just two arguments and let infix notation take care of the rest.
This is similar to the way `+` is a function of two variables, yet we can still write `a + b + c` without parentheses.
Associativity tells us the grouping doesn't matter.
Now that's also functional programming!

